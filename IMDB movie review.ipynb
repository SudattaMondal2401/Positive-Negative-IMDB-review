{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd1c818",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63513430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bc02f",
   "metadata": {},
   "source": [
    "# Uploading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe0d7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\\\Work\\\\My work\\\\Imdb review analysis\\\\archive\\\\IMDB Dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47c265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f05926",
   "metadata": {},
   "source": [
    "# Preliminary viewing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8379f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1037739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7482c171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1fa31",
   "metadata": {},
   "source": [
    "# Checking for null values and empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708c4dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02456a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = []\n",
    "for i, rv, s in dataset.itertuples():\n",
    "    if rv.isspace()==True:\n",
    "        blank.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2688b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8f58a",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cd4c6",
   "metadata": {},
   "source": [
    "### Into matrix of features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749a340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:, 0]\n",
    "y = dataset.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c796e4",
   "metadata": {},
   "source": [
    "### Applying Spacy preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf74421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c09a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    doc = nlp(text)\n",
    "    lemma = []\n",
    "    for token in doc:\n",
    "        if token.is_stop==False:\n",
    "            lemma.append(token.lemma_)\n",
    "    processed_text = \" \".join(lemma)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03e50842",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db4509",
   "metadata": {},
   "source": [
    "### Dividing into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b789dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d54b2a",
   "metadata": {},
   "source": [
    "# Applying transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8bfd412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "y_train_encoded = lb.fit_transform(y_train)\n",
    "y_test_encoded = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4791bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_encoded = vectorizer.fit_transform(x_train)\n",
    "x_test_encoded = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79396820",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c0e474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841efc1",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "782e09e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[7278 2728]\n",
      " [2913 7081]]\n",
      "The f1 score is 0.7151441700752412\n",
      "The accuracy score is 0.71795\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72     10006\n",
      "           1       0.72      0.71      0.72      9994\n",
      "\n",
      "    accuracy                           0.72     20000\n",
      "   macro avg       0.72      0.72      0.72     20000\n",
      "weighted avg       0.72      0.72      0.72     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier1 = DecisionTreeClassifier()\n",
    "classifier1.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier1.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e956f",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b4c26ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8728 1278]\n",
      " [1061 8933]]\n",
      "The f1 score is 0.8842365751051721\n",
      "The accuracy score is 0.88305\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88     10006\n",
      "           1       0.87      0.89      0.88      9994\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.88      0.88      0.88     20000\n",
      "weighted avg       0.88      0.88      0.88     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier2 = LogisticRegression()\n",
    "classifier2.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier2.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f36b2",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4dc2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8523 1483]\n",
      " [1589 8405]]\n",
      "The f1 score is 0.8454883814505584\n",
      "The accuracy score is 0.8464\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85     10006\n",
      "           1       0.85      0.84      0.85      9994\n",
      "\n",
      "    accuracy                           0.85     20000\n",
      "   macro avg       0.85      0.85      0.85     20000\n",
      "weighted avg       0.85      0.85      0.85     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier3 = RandomForestClassifier(n_jobs=-1)\n",
    "classifier3.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier3.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059fe1b",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e801083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8697 1309]\n",
      " [1601 8393]]\n",
      "The f1 score is 0.8522542648253453\n",
      "The accuracy score is 0.8545\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86     10006\n",
      "           1       0.87      0.84      0.85      9994\n",
      "\n",
      "    accuracy                           0.85     20000\n",
      "   macro avg       0.85      0.85      0.85     20000\n",
      "weighted avg       0.85      0.85      0.85     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier4 = MultinomialNB()\n",
    "classifier4.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier4.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f964b00",
   "metadata": {},
   "source": [
    "### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac48feb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[7394 2612]\n",
      " [2162 7832]]\n",
      "The f1 score is 0.7664155005382131\n",
      "The accuracy score is 0.7613\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.76     10006\n",
      "           1       0.75      0.78      0.77      9994\n",
      "\n",
      "    accuracy                           0.76     20000\n",
      "   macro avg       0.76      0.76      0.76     20000\n",
      "weighted avg       0.76      0.76      0.76     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier5 = KNeighborsClassifier()\n",
    "classifier5.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier5.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0de259",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63596adb",
   "metadata": {},
   "source": [
    "### Linear support vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "092343b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8817 1189]\n",
      " [1127 8867]]\n",
      "The f1 score is 0.8844887780548629\n",
      "The accuracy score is 0.8842\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88     10006\n",
      "           1       0.88      0.89      0.88      9994\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.88      0.88      0.88     20000\n",
      "weighted avg       0.88      0.88      0.88     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "classifier6 = LinearSVC()\n",
    "classifier6.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier6.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b72068",
   "metadata": {},
   "source": [
    "### Sigmoid SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8601fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8773 1233]\n",
      " [1085 8909]]\n",
      "The f1 score is 0.8848827969805324\n",
      "The accuracy score is 0.8841\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88     10006\n",
      "           1       0.88      0.89      0.88      9994\n",
      "\n",
      "    accuracy                           0.88     20000\n",
      "   macro avg       0.88      0.88      0.88     20000\n",
      "weighted avg       0.88      0.88      0.88     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier7 = SVC(kernel = 'sigmoid')\n",
    "classifier7.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier7.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ba4a5",
   "metadata": {},
   "source": [
    "### Voting Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1336f",
   "metadata": {},
   "source": [
    "##### Soft Voting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "799563cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8830 1176]\n",
      " [1059 8935]]\n",
      "The f1 score is 0.888833623476747\n",
      "The accuracy score is 0.88825\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89     10006\n",
      "           1       0.88      0.89      0.89      9994\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.89      0.89      0.89     20000\n",
      "weighted avg       0.89      0.89      0.89     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "classifier8 = VotingClassifier(\n",
    "    estimators=[('lr', LogisticRegression()), ('svc', SVC(kernel = 'sigmoid', probability = True)), ('rf', RandomForestClassifier())],\n",
    "    voting='soft')\n",
    "classifier8.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier8.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16d358",
   "metadata": {},
   "source": [
    "##### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07a03cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8813 1193]\n",
      " [1081 8913]]\n",
      "The f1 score is 0.886865671641791\n",
      "The accuracy score is 0.8863\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89     10006\n",
      "           1       0.88      0.89      0.89      9994\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.89      0.89      0.89     20000\n",
      "weighted avg       0.89      0.89      0.89     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "classifier9 = VotingClassifier(\n",
    " estimators=[('lr', LogisticRegression()), ('svc', LinearSVC()), ('rf', RandomForestClassifier())],\n",
    "    voting='hard')\n",
    "classifier9.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier9.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8ecef",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38c488bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix is given by: \n",
      "[[8905 1101]\n",
      " [1101 8893]]\n",
      "The f1 score is 0.889833900340204\n",
      "The accuracy score is 0.8899\n",
      "The classification report is: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89     10006\n",
      "           1       0.89      0.89      0.89      9994\n",
      "\n",
      "    accuracy                           0.89     20000\n",
      "   macro avg       0.89      0.89      0.89     20000\n",
      "weighted avg       0.89      0.89      0.89     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression()))\n",
    "level0.append(('svm', LinearSVC()))\n",
    "level0.append(('bayes', MultinomialNB()))\n",
    "level0.append(('random', RandomForestClassifier()))\n",
    "\n",
    "level1 = LogisticRegression()\n",
    "\n",
    "classifier10 = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "classifier10.fit(x_train_encoded, y_train_encoded)\n",
    "\n",
    "y_pred = classifier10.predict(x_test_encoded)\n",
    "print(\"The confusion matrix is given by: \")\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "print(f'The f1 score is {f1_score(y_test_encoded, y_pred)}')\n",
    "print(f'The accuracy score is {accuracy_score(y_test_encoded, y_pred)}')\n",
    "print(\"The classification report is: \")\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
